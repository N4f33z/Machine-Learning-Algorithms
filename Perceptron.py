# -*- coding: utf-8 -*-
"""Perceptron-Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pw2GCtr1JXEbgUZbsHKwe0_vE4uI-7iE
"""

import numpy as np
import pandas as pd
import random
import sys
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from google.colab import files

upload = files.upload()

df = pd.read_csv("irisBinaryClasssification.csv", header=None)
#inputs = df.iloc[0:150, [0, 1, 2, 3]].values
#original_classes = df.iloc[0:150, 4].values

#Added columns as header
df.columns = df.iloc[0]
df = df[1:]

#selecting the features and target variables
x = df.iloc[0:100,[0, 1, 2, 3]].values
y = df.iloc[0:100,4].values
y = np.where(y=='Iris-setosa', 0, 1) #Changing the target values to integer classes

#Test Function
def test(x_test,w,b):
  result = []
  for i in x_test:
    temp = sigmoid(np.dot(i, w) + b)
    if temp<0.5:
      result.append(0)
    else:
      result.append(1)
  return result

#Sigmoid Function
def sigmoid(x):
  sigmoid = 1/(1+np.exp(-x)) #https://mathworld.wolfram.com/SigmoidFunction.html
  return sigmoid

#Cost function
def cost_function(CostbyPrediction,sig):
  predictionbysigma = sigmoid(sig)*(1-sigmoid(sig)) #Derivative of sigmoid function
  derivative = predictionbysigma*CostbyPrediction #calculated by Cost by Prediction and Prediction in terms of sigmoid value
  return derivative

def perceptron(feature, label, weights, bias, learningrate,epochs):
  for epoch in range(epochs): #Running for 10/100 epochs
      feed = np.dot(np.array(feature,dtype=float), weights) + bias #dot product of weight with feature in addition to bias 
      sig = sigmoid(feed) #first Predicted output

      # Backpropagation section:
      error = sig - label #calculating eorr from precdicted and actual class
      derivative = cost_function(error,sig) #Calculate differentiation of each predicted class through cost function
      #print(derivative)
      features = np.dot(np.array(feature.T,dtype=float), derivative)
      weights = weights - learningrate*features #increasing the speed of convergence with learning rate and update weight

      for i in derivative:
          bias = bias - learningrate*i #update bias in respect to each derivatives

  return bias, weights

def driver(learningrate, epochs):

  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=17) #We split the dataset
  x_train = np.array(x_train,dtype=float)  
  x_test = np.array(x_test,dtype=float)
  y_train = np.array(y_train,dtype=float)
  y_test = np.array(y_test,dtype=float)

  weights = [0.1, 0.1, 0.1, 0.1] 
  bias = 0.1 
  #learningrate = 0.1

  updated_bias, updated_weight = perceptron(x_train, y_train, weights, bias, learningrate, epochs)
  result = test(x_test,updated_weight,updated_bias)
  print("LearningRate: ", learningrate, "Epoch: ",epochs)
  print("Predicted Class: ", result)
  print("Actual Class", y_test)
  print("=====================================")
  print(f"Accuracy: {sum(result == y_test) / y_test.shape[0]*100}%")

lr = [0.01, 0.1, 1]
epochs = [10,100]
for i in lr:
  for j in epochs:
    driver(i,j)

